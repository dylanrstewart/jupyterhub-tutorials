{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Language Models: A Hands-On Tutorial\n",
    "\n",
    "Welcome! This notebook walks you through the theory and practice of **Recursive Language Models (RLMs)** — models that process language by exploiting its inherent hierarchical, tree-structured nature rather than treating it as a flat sequence.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Background & Motivation](#1-background--motivation)\n",
    "2. [From Sequences to Trees: Recursive Structure in Language](#2-from-sequences-to-trees)\n",
    "3. [Recursive Neural Networks (TreeRNNs)](#3-recursive-neural-networks)\n",
    "4. [Building a Recursive Neural Network from Scratch](#4-building-a-recursive-neural-network-from-scratch)\n",
    "5. [Training on Sentiment Treebank](#5-training-on-sentiment-treebank)\n",
    "6. [Tree-LSTMs: Adding Memory to Recursion](#6-tree-lstms)\n",
    "7. [Recursive Transformers & Modern Approaches](#7-recursive-transformers--modern-approaches)\n",
    "8. [Visualizing Learned Representations](#8-visualizing-learned-representations)\n",
    "9. [Exercises](#9-exercises)\n",
    "10. [References](#10-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Background & Motivation <a id='1-background--motivation'></a>\n",
    "\n",
    "Natural language has **recursive** structure. Consider the sentence:\n",
    "\n",
    "> *\"The cat that the dog chased ran away.\"*\n",
    "\n",
    "The relative clause *\"that the dog chased\"* is embedded inside the main clause. Human languages routinely nest phrases inside phrases, creating tree-structured representations.\n",
    "\n",
    "### Why does this matter for language models?\n",
    "\n",
    "| Approach | Strengths | Weaknesses |\n",
    "|----------|-----------|------------|\n",
    "| **n-gram / Bag-of-Words** | Simple, fast | Ignores word order and structure |\n",
    "| **RNN / LSTM (sequential)** | Captures order | Struggles with long-range hierarchical deps |\n",
    "| **Transformer (sequential)** | Attention over all positions | Quadratic cost; structure is implicit |\n",
    "| **Recursive Neural Net** | Explicitly models tree structure | Needs parse trees; harder to batch |\n",
    "\n",
    "Recursive Language Models bridge the gap by composing meaning **bottom-up** through a syntactic parse tree, mirroring how linguists believe meaning is constructed in natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. From Sequences to Trees: Recursive Structure in Language <a id='2-from-sequences-to-trees'></a>\n",
    "\n",
    "A **constituency parse tree** breaks a sentence into nested constituents:\n",
    "\n",
    "```\n",
    "          S\n",
    "         / \\\n",
    "        NP   VP\n",
    "       / \\    / \\\n",
    "     The cat  V   NP\n",
    "              |   / \\\n",
    "             sat the  mat\n",
    "```\n",
    "\n",
    "A Recursive Language Model assigns a vector to each node by composing child vectors bottom-up:\n",
    "\n",
    "1. Leaf nodes get word embeddings: $\\mathbf{x}_{\\text{cat}}, \\mathbf{x}_{\\text{sat}}, \\ldots$\n",
    "2. Internal nodes compose children: $\\mathbf{h}_{\\text{parent}} = f(\\mathbf{h}_{\\text{left}}, \\mathbf{h}_{\\text{right}})$\n",
    "3. The root vector represents the entire sentence.\n",
    "\n",
    "Let's start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Recursive Neural Networks (TreeRNNs) <a id='3-recursive-neural-networks'></a>\n",
    "\n",
    "The simplest Recursive Neural Network (Socher et al., 2011) uses a single composition function shared across all nodes:\n",
    "\n",
    "$$\\mathbf{h}_{\\text{parent}} = \\tanh\\!\\left(W \\begin{bmatrix} \\mathbf{h}_{\\text{left}} \\\\ \\mathbf{h}_{\\text{right}} \\end{bmatrix} + \\mathbf{b}\\right)$$\n",
    "\n",
    "where $W \\in \\mathbb{R}^{d \\times 2d}$ and $\\mathbf{b} \\in \\mathbb{R}^{d}$.\n",
    "\n",
    "### Key insight\n",
    "The same weight matrix $W$ is applied **recursively** at every node — the model learns a single, universal composition function that works at every level of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Building a Recursive Neural Network from Scratch <a id='4-building-a-recursive-neural-network-from-scratch'></a>\n",
    "\n",
    "### 4.1 Tree Data Structure\n",
    "\n",
    "We first need a tree representation. We'll use a simple binary tree where each node stores either a word (leaf) or has two children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    \"\"\"Binary tree node for recursive neural networks.\"\"\"\n",
    "    def __init__(self, word=None, left=None, right=None, label=None):\n",
    "        self.word = word        # Only for leaf nodes\n",
    "        self.left = left        # Left child Tree\n",
    "        self.right = right      # Right child Tree\n",
    "        self.label = label      # Sentiment label (0-4)\n",
    "        self.state = None       # Hidden state (filled during forward pass)\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.word is not None\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.is_leaf:\n",
    "            return f'Tree(\"{self.word}\")'\n",
    "        return f'Tree({self.left}, {self.right})'\n",
    "\n",
    "\n",
    "def build_example_tree():\n",
    "    \"\"\"Build: (The (cat sat))\"\"\"\n",
    "    the = Tree(word=\"The\", label=2)\n",
    "    cat = Tree(word=\"cat\", label=2)\n",
    "    sat = Tree(word=\"sat\", label=2)\n",
    "    cat_sat = Tree(left=cat, right=sat, label=3)\n",
    "    root = Tree(left=the, right=cat_sat, label=3)\n",
    "    return root\n",
    "\n",
    "\n",
    "tree = build_example_tree()\n",
    "print(\"Example tree:\", tree)\n",
    "print(\"Root is leaf?\", tree.is_leaf)\n",
    "print(\"Left child:\", tree.left)\n",
    "print(\"Right child:\", tree.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Recursive Neural Network Module\n",
    "\n",
    "Now we implement the core TreeRNN model in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveNN(nn.Module):\n",
    "    \"\"\"Vanilla Recursive Neural Network for sentiment classification.\n",
    "    \n",
    "    Composes word embeddings bottom-up through a binary parse tree\n",
    "    using a single shared composition function.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, vocab=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # The recursive composition function:\n",
    "        # h_parent = tanh(W [h_left; h_right] + b)\n",
    "        self.W = nn.Linear(2 * embed_dim, embed_dim)\n",
    "        \n",
    "        # Classification head (applied at every node)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Vocabulary mapping\n",
    "        self.vocab = vocab or {}\n",
    "        self.unk_idx = 0\n",
    "    \n",
    "    def forward_node(self, tree):\n",
    "        \"\"\"Recursively compute the hidden state for a tree node.\"\"\"\n",
    "        if tree.is_leaf:\n",
    "            # Leaf: look up embedding\n",
    "            idx = self.vocab.get(tree.word.lower(), self.unk_idx)\n",
    "            idx_tensor = torch.tensor([idx], device=device)\n",
    "            tree.state = self.embedding(idx_tensor).squeeze(0)\n",
    "        else:\n",
    "            # Internal node: recursively compute children, then compose\n",
    "            left_state = self.forward_node(tree.left)\n",
    "            right_state = self.forward_node(tree.right)\n",
    "            combined = torch.cat([left_state, right_state], dim=0)\n",
    "            tree.state = torch.tanh(self.W(combined))\n",
    "        return tree.state\n",
    "\n",
    "    def forward(self, tree):\n",
    "        \"\"\"Compute all node states and return logits for every node.\"\"\"\n",
    "        all_nodes = []\n",
    "        self._collect_nodes(tree, all_nodes)\n",
    "        \n",
    "        # Forward pass: compute root state (recursion handles all nodes)\n",
    "        self.forward_node(tree)\n",
    "        \n",
    "        # Collect states and labels for all nodes\n",
    "        states = torch.stack([n.state for n in all_nodes])\n",
    "        logits = self.classifier(states)\n",
    "        labels = torch.tensor([n.label for n in all_nodes], device=device)\n",
    "        return logits, labels\n",
    "\n",
    "    def _collect_nodes(self, tree, nodes):\n",
    "        \"\"\"Collect all nodes in the tree (post-order traversal).\"\"\"\n",
    "        if not tree.is_leaf:\n",
    "            self._collect_nodes(tree.left, nodes)\n",
    "            self._collect_nodes(tree.right, nodes)\n",
    "        nodes.append(tree)\n",
    "\n",
    "\n",
    "print(\"RecursiveNN class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Understanding the Forward Pass\n",
    "\n",
    "Let's visualize what happens when we pass our example tree through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small vocabulary and model for demonstration\n",
    "demo_vocab = {\"the\": 1, \"cat\": 2, \"sat\": 3, \"on\": 4, \"mat\": 5}\n",
    "demo_model = RecursiveNN(\n",
    "    vocab_size=100,\n",
    "    embed_dim=8,\n",
    "    num_classes=5,\n",
    "    vocab=demo_vocab\n",
    ").to(device)\n",
    "\n",
    "# Forward pass on example tree\n",
    "tree = build_example_tree()\n",
    "with torch.no_grad():\n",
    "    logits, labels = demo_model(tree)\n",
    "\n",
    "print(\"Number of nodes:\", logits.shape[0])\n",
    "print(\"\\nNode states (hidden vectors):\")\n",
    "all_nodes = []\n",
    "demo_model._collect_nodes(tree, all_nodes)\n",
    "for node in all_nodes:\n",
    "    name = node.word if node.is_leaf else \"[internal]\"\n",
    "    print(f\"  {name:12s} -> state shape: {node.state.shape}, \"\n",
    "          f\"first 4 values: {node.state[:4].cpu().numpy().round(3)}\")\n",
    "\n",
    "print(f\"\\nLogits shape: {logits.shape}  (nodes x classes)\")\n",
    "print(f\"Labels: {labels.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training on Sentiment Treebank <a id='5-training-on-sentiment-treebank'></a>\n",
    "\n",
    "The **Stanford Sentiment Treebank (SST)** is the classic dataset for recursive models. Each node in a binary parse tree has a sentiment label from 0 (very negative) to 4 (very positive).\n",
    "\n",
    "We'll work with a synthetic version to keep things self-contained and fast, then show how to load the real SST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# --- Synthetic Sentiment Treebank ---\n",
    "# We generate random binary trees with sentiment labels for demonstration.\n",
    "\n",
    "POSITIVE_WORDS = [\"great\", \"wonderful\", \"amazing\", \"excellent\", \"fantastic\",\n",
    "                  \"love\", \"beautiful\", \"brilliant\", \"superb\", \"outstanding\"]\n",
    "NEGATIVE_WORDS = [\"terrible\", \"awful\", \"horrible\", \"bad\", \"worst\",\n",
    "                  \"hate\", \"ugly\", \"boring\", \"dreadful\", \"poor\"]\n",
    "NEUTRAL_WORDS = [\"the\", \"a\", \"movie\", \"film\", \"this\", \"is\", \"was\",\n",
    "                 \"it\", \"very\", \"quite\", \"rather\", \"really\", \"but\"]\n",
    "NEGATION_WORDS = [\"not\", \"never\", \"no\", \"barely\", \"hardly\"]\n",
    "\n",
    "ALL_WORDS = POSITIVE_WORDS + NEGATIVE_WORDS + NEUTRAL_WORDS + NEGATION_WORDS\n",
    "VOCAB = {w: i + 1 for i, w in enumerate(ALL_WORDS)}  # 0 reserved for <unk>\n",
    "\n",
    "\n",
    "def make_random_tree(depth=0, max_depth=3):\n",
    "    \"\"\"Generate a random binary sentiment tree.\"\"\"\n",
    "    if depth >= max_depth or (depth > 0 and random.random() < 0.4):\n",
    "        # Leaf node\n",
    "        category = random.choice(['pos', 'neg', 'neu', 'negation'])\n",
    "        if category == 'pos':\n",
    "            word = random.choice(POSITIVE_WORDS)\n",
    "            label = random.choice([3, 4])\n",
    "        elif category == 'neg':\n",
    "            word = random.choice(NEGATIVE_WORDS)\n",
    "            label = random.choice([0, 1])\n",
    "        elif category == 'negation':\n",
    "            word = random.choice(NEGATION_WORDS)\n",
    "            label = 2\n",
    "        else:\n",
    "            word = random.choice(NEUTRAL_WORDS)\n",
    "            label = 2\n",
    "        return Tree(word=word, label=label)\n",
    "    else:\n",
    "        left = make_random_tree(depth + 1, max_depth)\n",
    "        right = make_random_tree(depth + 1, max_depth)\n",
    "        # Parent label is a rough combination\n",
    "        avg = (left.label + right.label) / 2\n",
    "        label = int(round(avg + random.uniform(-0.5, 0.5)))\n",
    "        label = max(0, min(4, label))\n",
    "        return Tree(left=left, right=right, label=label)\n",
    "\n",
    "\n",
    "# Generate training and validation data\n",
    "random.seed(42)\n",
    "train_trees = [make_random_tree() for _ in range(2000)]\n",
    "val_trees = [make_random_tree() for _ in range(400)]\n",
    "\n",
    "def count_nodes(tree):\n",
    "    if tree.is_leaf:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree.left) + count_nodes(tree.right)\n",
    "\n",
    "total_train_nodes = sum(count_nodes(t) for t in train_trees)\n",
    "print(f\"Training trees: {len(train_trees)}\")\n",
    "print(f\"Validation trees: {len(val_trees)}\")\n",
    "print(f\"Total training nodes: {total_train_nodes}\")\n",
    "print(f\"Average nodes per tree: {total_train_nodes / len(train_trees):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Training Loop\n",
    "\n",
    "Because trees have variable structure, we process one tree at a time (no easy batching for vanilla TreeRNNs — this is one of their practical limitations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, trees, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    random.shuffle(trees)\n",
    "    for tree in trees:\n",
    "        optimizer.zero_grad()\n",
    "        logits, labels = model(tree)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        # Gradient clipping to avoid exploding gradients in deep trees\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * labels.shape[0]\n",
    "        total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        total_nodes += labels.shape[0]\n",
    "    \n",
    "    return total_loss / total_nodes, total_correct / total_nodes\n",
    "\n",
    "\n",
    "def evaluate(model, trees, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    root_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tree in trees:\n",
    "            logits, labels = model(tree)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * labels.shape[0]\n",
    "            total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "            total_nodes += labels.shape[0]\n",
    "            # Root accuracy (last node in post-order traversal)\n",
    "            root_correct += (logits[-1].argmax() == labels[-1]).item()\n",
    "    \n",
    "    return (total_loss / total_nodes,\n",
    "            total_correct / total_nodes,\n",
    "            root_correct / len(trees))\n",
    "\n",
    "\n",
    "print(\"Training functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the Recursive Neural Network ---\n",
    "\n",
    "EMBED_DIM = 64\n",
    "NUM_CLASSES = 5\n",
    "LR = 0.01\n",
    "EPOCHS = 8\n",
    "\n",
    "model = RecursiveNN(\n",
    "    vocab_size=len(VOCAB) + 1,  # +1 for <unk>\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    vocab=VOCAB\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [],\n",
    "           'val_loss': [], 'val_acc': [], 'val_root_acc': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_trees, optimizer, criterion)\n",
    "    val_loss, val_acc, val_root_acc = evaluate(model, val_trees, criterion)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_root_acc'].append(val_root_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.3f} Root: {val_root_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Training Curves ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curves')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Node-Level Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].plot(history['val_root_acc'], label='Root Acc', color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].set_title('Root (Sentence) Accuracy')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Tree-LSTMs: Adding Memory to Recursion <a id='6-tree-lstms'></a>\n",
    "\n",
    "Just as LSTMs improved upon vanilla RNNs for sequences, **Tree-LSTMs** (Tai et al., 2015) add gating mechanisms to recursive composition.\n",
    "\n",
    "### Child-Sum Tree-LSTM\n",
    "\n",
    "For a node $j$ with children $C(j)$:\n",
    "\n",
    "$$\\tilde{h}_j = \\sum_{k \\in C(j)} h_k$$\n",
    "\n",
    "$$i_j = \\sigma(W^{(i)} x_j + U^{(i)} \\tilde{h}_j + b^{(i)})$$\n",
    "\n",
    "$$f_{jk} = \\sigma(W^{(f)} x_j + U^{(f)} h_k + b^{(f)}) \\quad \\forall k \\in C(j)$$\n",
    "\n",
    "$$o_j = \\sigma(W^{(o)} x_j + U^{(o)} \\tilde{h}_j + b^{(o)})$$\n",
    "\n",
    "$$u_j = \\tanh(W^{(u)} x_j + U^{(u)} \\tilde{h}_j + b^{(u)})$$\n",
    "\n",
    "$$c_j = i_j \\odot u_j + \\sum_{k \\in C(j)} f_{jk} \\odot c_k$$\n",
    "\n",
    "$$h_j = o_j \\odot \\tanh(c_j)$$\n",
    "\n",
    "### Binary Tree-LSTM (N-ary Tree-LSTM)\n",
    "\n",
    "For binary trees specifically, we have separate parameters for left and right children:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryTreeLSTM(nn.Module):\n",
    "    \"\"\"Binary Tree-LSTM for sentiment classification.\n",
    "    \n",
    "    Each internal node composes its left and right children using\n",
    "    LSTM-style gating, allowing the model to learn what to remember\n",
    "    and forget from each subtree.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, vocab=None):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Leaf transformation\n",
    "        self.leaf_linear = nn.Linear(embed_dim, hidden_dim)\n",
    "        \n",
    "        # Tree-LSTM gates for binary composition\n",
    "        # Input gate\n",
    "        self.U_i_l = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.U_i_r = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Forget gates (one per child)\n",
    "        self.U_fl_l = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.U_fl_r = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.U_fr_l = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.U_fr_r = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Output gate\n",
    "        self.U_o_l = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.U_o_r = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Cell update\n",
    "        self.U_u_l = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.U_u_r = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        self.vocab = vocab or {}\n",
    "        self.unk_idx = 0\n",
    "    \n",
    "    def forward_node(self, tree):\n",
    "        \"\"\"Recursively compute (h, c) for a tree node.\"\"\"\n",
    "        if tree.is_leaf:\n",
    "            idx = self.vocab.get(tree.word.lower(), self.unk_idx)\n",
    "            idx_tensor = torch.tensor([idx], device=device)\n",
    "            emb = self.embedding(idx_tensor).squeeze(0)\n",
    "            h = torch.tanh(self.leaf_linear(emb))\n",
    "            c = torch.zeros(self.hidden_dim, device=device)\n",
    "            tree.state = h\n",
    "            tree.cell = c\n",
    "        else:\n",
    "            self.forward_node(tree.left)\n",
    "            self.forward_node(tree.right)\n",
    "            h_l, c_l = tree.left.state, tree.left.cell\n",
    "            h_r, c_r = tree.right.state, tree.right.cell\n",
    "            \n",
    "            i = torch.sigmoid(self.U_i_l(h_l) + self.U_i_r(h_r))\n",
    "            f_l = torch.sigmoid(self.U_fl_l(h_l) + self.U_fl_r(h_r))\n",
    "            f_r = torch.sigmoid(self.U_fr_l(h_l) + self.U_fr_r(h_r))\n",
    "            o = torch.sigmoid(self.U_o_l(h_l) + self.U_o_r(h_r))\n",
    "            u = torch.tanh(self.U_u_l(h_l) + self.U_u_r(h_r))\n",
    "            \n",
    "            c = i * u + f_l * c_l + f_r * c_r\n",
    "            h = o * torch.tanh(c)\n",
    "            \n",
    "            tree.state = h\n",
    "            tree.cell = c\n",
    "        \n",
    "        return tree.state\n",
    "    \n",
    "    def forward(self, tree):\n",
    "        all_nodes = []\n",
    "        self._collect_nodes(tree, all_nodes)\n",
    "        self.forward_node(tree)\n",
    "        states = torch.stack([n.state for n in all_nodes])\n",
    "        logits = self.classifier(states)\n",
    "        labels = torch.tensor([n.label for n in all_nodes], device=device)\n",
    "        return logits, labels\n",
    "    \n",
    "    def _collect_nodes(self, tree, nodes):\n",
    "        if not tree.is_leaf:\n",
    "            self._collect_nodes(tree.left, nodes)\n",
    "            self._collect_nodes(tree.right, nodes)\n",
    "        nodes.append(tree)\n",
    "\n",
    "\n",
    "print(\"BinaryTreeLSTM class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Tree-LSTM ---\n",
    "\n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "tree_lstm = BinaryTreeLSTM(\n",
    "    vocab_size=len(VOCAB) + 1,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    vocab=VOCAB\n",
    ").to(device)\n",
    "\n",
    "optimizer_lstm = torch.optim.Adam(tree_lstm.parameters(), lr=0.005)\n",
    "criterion_lstm = nn.CrossEntropyLoss()\n",
    "\n",
    "lstm_history = {'train_loss': [], 'train_acc': [],\n",
    "                'val_loss': [], 'val_acc': [], 'val_root_acc': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Reuse the same train/evaluate functions — they work with any model\n",
    "    # that implements forward(tree) -> (logits, labels)\n",
    "    train_loss, train_acc = train_epoch(tree_lstm, train_trees, optimizer_lstm, criterion_lstm)\n",
    "    val_loss, val_acc, val_root_acc = evaluate(tree_lstm, val_trees, criterion_lstm)\n",
    "    \n",
    "    lstm_history['train_loss'].append(train_loss)\n",
    "    lstm_history['train_acc'].append(train_acc)\n",
    "    lstm_history['val_loss'].append(val_loss)\n",
    "    lstm_history['val_acc'].append(val_acc)\n",
    "    lstm_history['val_root_acc'].append(val_root_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.3f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.3f} Root: {val_root_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare TreeRNN vs Tree-LSTM ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['val_acc'], label='TreeRNN', marker='o')\n",
    "axes[0].plot(lstm_history['val_acc'], label='Tree-LSTM', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Validation Node Accuracy')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['val_root_acc'], label='TreeRNN', marker='o')\n",
    "axes[1].plot(lstm_history['val_root_acc'], label='Tree-LSTM', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Root (Sentence) Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Recursive Transformers & Modern Approaches <a id='7-recursive-transformers--modern-approaches'></a>\n",
    "\n",
    "While classic TreeRNNs/Tree-LSTMs require explicit parse trees, modern approaches blend recursive ideas with Transformers:\n",
    "\n",
    "### 7.1 Approaches Overview\n",
    "\n",
    "| Method | Key Idea | Parse Tree Required? |\n",
    "|--------|----------|---------------------|\n",
    "| **TreeRNN** (Socher, 2011) | Single composition function | Yes |\n",
    "| **Tree-LSTM** (Tai, 2015) | Gated composition | Yes |\n",
    "| **SPINN** (Bowman, 2016) | Stack-based shift-reduce parser + composition | Uses parser |\n",
    "| **Ordered Neurons** (Shen, 2019) | LSTM with implicit tree structure via ordered gates | No |\n",
    "| **Tree Transformer** (Wang, 2019) | Attention constrained by constituency trees | Yes |\n",
    "| **CRvNN** (Choi, 2018) | Learns to compose with RL (no given tree) | No (learned) |\n",
    "| **R2D2** (Hu, 2021) | Recursive Transformer with CKY-style dynamic programming | No (learned) |\n",
    "\n",
    "### 7.2 A Simple Recursive Transformer Block\n",
    "\n",
    "Let's implement a composition function that uses multi-head attention instead of a simple linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveTransformerComposer(nn.Module):\n",
    "    \"\"\"Compose two child vectors using a Transformer-style attention block.\n",
    "    \n",
    "    Instead of h_parent = tanh(W [h_l; h_r] + b), we treat the two\n",
    "    child states as a length-2 sequence and apply self-attention + FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead=4, dim_feedforward=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, h_left, h_right):\n",
    "        # Stack children as a length-2 sequence: [batch=1, seq=2, d_model]\n",
    "        x = torch.stack([h_left, h_right], dim=0).unsqueeze(0)\n",
    "        \n",
    "        # Self-attention over the two children\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        # Pool: mean of the two attended representations\n",
    "        return x.squeeze(0).mean(dim=0)\n",
    "\n",
    "\n",
    "class RecursiveTransformer(nn.Module):\n",
    "    \"\"\"Full Recursive Transformer model for tree-structured classification.\"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_classes, nhead=4, vocab=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.composer = RecursiveTransformerComposer(d_model, nhead=nhead)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.vocab = vocab or {}\n",
    "        self.unk_idx = 0\n",
    "    \n",
    "    def forward_node(self, tree):\n",
    "        if tree.is_leaf:\n",
    "            idx = self.vocab.get(tree.word.lower(), self.unk_idx)\n",
    "            idx_tensor = torch.tensor([idx], device=device)\n",
    "            tree.state = self.embedding(idx_tensor).squeeze(0)\n",
    "        else:\n",
    "            self.forward_node(tree.left)\n",
    "            self.forward_node(tree.right)\n",
    "            tree.state = self.composer(tree.left.state, tree.right.state)\n",
    "        return tree.state\n",
    "    \n",
    "    def forward(self, tree):\n",
    "        all_nodes = []\n",
    "        self._collect_nodes(tree, all_nodes)\n",
    "        self.forward_node(tree)\n",
    "        states = torch.stack([n.state for n in all_nodes])\n",
    "        logits = self.classifier(states)\n",
    "        labels = torch.tensor([n.label for n in all_nodes], device=device)\n",
    "        return logits, labels\n",
    "    \n",
    "    def _collect_nodes(self, tree, nodes):\n",
    "        if not tree.is_leaf:\n",
    "            self._collect_nodes(tree.left, nodes)\n",
    "            self._collect_nodes(tree.right, nodes)\n",
    "        nodes.append(tree)\n",
    "\n",
    "\n",
    "# Quick test\n",
    "rt_model = RecursiveTransformer(\n",
    "    vocab_size=len(VOCAB) + 1,\n",
    "    d_model=64,\n",
    "    num_classes=5,\n",
    "    nhead=4,\n",
    "    vocab=VOCAB\n",
    ").to(device)\n",
    "\n",
    "test_tree = build_example_tree()\n",
    "with torch.no_grad():\n",
    "    logits, labels = rt_model(test_tree)\n",
    "print(f\"Recursive Transformer output shape: {logits.shape}\")\n",
    "print(\"Model parameters:\", sum(p.numel() for p in rt_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visualizing Learned Representations <a id='8-visualizing-learned-representations'></a>\n",
    "\n",
    "One of the most compelling aspects of recursive models is that every node in the tree gets a vector representation — we can visualize how sentiment is composed through the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree_sentiment(model, tree, ax=None):\n",
    "    \"\"\"Visualize predicted sentiment at each node of a tree.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, labels = model(tree)\n",
    "    \n",
    "    all_nodes = []\n",
    "    model._collect_nodes(tree, all_nodes)\n",
    "    \n",
    "    predictions = logits.argmax(dim=1).cpu().numpy()\n",
    "    true_labels = labels.cpu().numpy()\n",
    "    probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Build a text representation showing predictions\n",
    "    sentiment_map = {0: 'V-Neg', 1: 'Neg', 2: 'Neutral', 3: 'Pos', 4: 'V-Pos'}\n",
    "    color_map = {0: '#d73027', 1: '#fc8d59', 2: '#ffffbf', 3: '#91bfdb', 4: '#4575b4'}\n",
    "    \n",
    "    # Create a simple visualization\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 3))\n",
    "    \n",
    "    for i, node in enumerate(all_nodes):\n",
    "        name = node.word if node.is_leaf else 'Node'\n",
    "        pred = predictions[i]\n",
    "        true = true_labels[i]\n",
    "        conf = probs[i][pred]\n",
    "        \n",
    "        color = color_map[pred]\n",
    "        ax.barh(i, conf, color=color, edgecolor='gray', height=0.7)\n",
    "        ax.text(conf + 0.02, i,\n",
    "                f'{name} | pred: {sentiment_map[pred]} ({conf:.0%}) | true: {sentiment_map[true]}',\n",
    "                va='center', fontsize=9)\n",
    "    \n",
    "    ax.set_xlim(0, 1.5)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_title('Per-Node Sentiment Predictions (post-order)')\n",
    "    ax.set_yticks(range(len(all_nodes)))\n",
    "    ax.set_yticklabels([n.word if n.is_leaf else '(compose)' for n in all_nodes])\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Visualize a few validation trees\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "    visualize_tree_sentiment(model, val_trees[i], ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize embeddings with t-SNE ---\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def collect_root_states(model, trees, max_trees=500):\n",
    "    \"\"\"Collect root hidden states and labels from a set of trees.\"\"\"\n",
    "    states = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tree in trees[:max_trees]:\n",
    "            model.forward_node(tree)\n",
    "            all_nodes = []\n",
    "            model._collect_nodes(tree, all_nodes)\n",
    "            # Root is the last node in post-order\n",
    "            states.append(all_nodes[-1].state.cpu().numpy())\n",
    "            labels.append(all_nodes[-1].label)\n",
    "    return np.array(states), np.array(labels)\n",
    "\n",
    "\n",
    "states, labels = collect_root_states(model, val_trees)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "coords = tsne.fit_transform(states)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sentiment_names = ['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive']\n",
    "colors = ['#d73027', '#fc8d59', '#ffffbf', '#91bfdb', '#4575b4']\n",
    "\n",
    "for label_idx in range(5):\n",
    "    mask = labels == label_idx\n",
    "    if mask.sum() > 0:\n",
    "        plt.scatter(coords[mask, 0], coords[mask, 1],\n",
    "                   c=colors[label_idx], label=sentiment_names[label_idx],\n",
    "                   alpha=0.7, edgecolors='gray', s=40)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('t-SNE of Root Node Representations (TreeRNN)')\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Exercises <a id='9-exercises'></a>\n",
    "\n",
    "Now it's your turn! Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Composition Function Variants\n",
    "Modify the `RecursiveNN` composition function to use:\n",
    "- (a) A **bilinear** composition: $h_p = \\tanh(h_l^T W h_r + b)$ (hint: use `nn.Bilinear`)\n",
    "- (b) An **MLP** with a hidden layer: $h_p = \\text{MLP}([h_l; h_r])$\n",
    "\n",
    "Compare their performance against the baseline.\n",
    "\n",
    "### Exercise 2: Sentiment Negation\n",
    "Create test trees where negation flips sentiment (e.g., \"not good\" should be negative). Test how well each model handles negation. Which architecture handles it best?\n",
    "\n",
    "### Exercise 3: Tree Depth Analysis  \n",
    "Measure how accuracy varies with tree depth. Do deeper trees pose more difficulty? Plot accuracy vs. tree depth for TreeRNN and Tree-LSTM.\n",
    "\n",
    "### Exercise 4: Real SST Data\n",
    "Load the actual Stanford Sentiment Treebank using HuggingFace datasets and train the models on it. You'll need to convert the SST format to our `Tree` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 1 Starter Code ---\n",
    "\n",
    "class RecursiveNN_MLP(nn.Module):\n",
    "    \"\"\"TODO: Implement a Recursive NN with a 2-layer MLP composition function.\n",
    "    \n",
    "    h_parent = MLP([h_left; h_right])\n",
    "    where MLP has one hidden layer with ReLU activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, vocab=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # self.composer = nn.Sequential(\n",
    "        #     nn.Linear(2 * embed_dim, hidden_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_dim, embed_dim),\n",
    "        # )\n",
    "        \n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "        self.vocab = vocab or {}\n",
    "        self.unk_idx = 0\n",
    "    \n",
    "    def forward_node(self, tree):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, tree):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "print(\"Exercise 1 starter code loaded. Uncomment and complete the TODOs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 2 Starter Code ---\n",
    "\n",
    "def create_negation_tests():\n",
    "    \"\"\"Create test cases where negation should flip sentiment.\"\"\"\n",
    "    tests = []\n",
    "    \n",
    "    # \"not great\" should be negative\n",
    "    not_node = Tree(word=\"not\", label=2)\n",
    "    great_node = Tree(word=\"great\", label=4)\n",
    "    not_great = Tree(left=not_node, right=great_node, label=1)\n",
    "    tests.append(('not great', not_great))\n",
    "    \n",
    "    # \"not terrible\" should be positive-ish\n",
    "    terrible_node = Tree(word=\"terrible\", label=0)\n",
    "    not_terrible = Tree(left=Tree(word=\"not\", label=2), right=terrible_node, label=3)\n",
    "    tests.append(('not terrible', not_terrible))\n",
    "    \n",
    "    # TODO: Add more negation test cases\n",
    "    \n",
    "    return tests\n",
    "\n",
    "\n",
    "negation_tests = create_negation_tests()\n",
    "print(\"Negation tests:\")\n",
    "for name, tree in negation_tests:\n",
    "    with torch.no_grad():\n",
    "        logits, labels = model(tree)\n",
    "        pred = logits[-1].argmax().item()  # root prediction\n",
    "        true = labels[-1].item()\n",
    "    sentiment_map = {0: 'V-Neg', 1: 'Neg', 2: 'Neutral', 3: 'Pos', 4: 'V-Pos'}\n",
    "    print(f\"  '{name}' -> predicted: {sentiment_map[pred]}, expected: {sentiment_map[true]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 4 Starter Code: Loading Real SST ---\n",
    "\n",
    "# Uncomment and run to load the Stanford Sentiment Treebank from HuggingFace\n",
    "# Note: This downloads the dataset (~30MB)\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# \n",
    "# sst = load_dataset(\"sst\", \"default\")\n",
    "# print(sst)\n",
    "# print(\"\\nExample:\")\n",
    "# print(sst['train'][0])\n",
    "# \n",
    "# # TODO: Convert SST bracket notation to Tree objects\n",
    "# # The SST stores trees in bracket notation like:\n",
    "# # (3 (2 (2 The) (2 Rock)) (4 (3 (2 is) ...) ...))\n",
    "# # Each number is the sentiment label (0-4)\n",
    "# \n",
    "# def parse_sst_tree(s):\n",
    "#     \"\"\"Parse SST bracket notation into our Tree class.\"\"\"\n",
    "#     # YOUR CODE HERE\n",
    "#     pass\n",
    "\n",
    "print(\"Exercise 4 starter code loaded. Uncomment to try with real SST data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. References <a id='10-references'></a>\n",
    "\n",
    "1. **Socher, R., et al.** (2011). *Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions.* EMNLP.\n",
    "\n",
    "2. **Socher, R., et al.** (2013). *Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.* EMNLP.\n",
    "\n",
    "3. **Tai, K.S., Socher, R., & Manning, C.D.** (2015). *Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.* ACL.\n",
    "\n",
    "4. **Bowman, S.R., et al.** (2016). *A Fast Unified Model for Parsing and Sentence Understanding.* ACL.\n",
    "\n",
    "5. **Shen, Y., et al.** (2019). *Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks.* ICLR.\n",
    "\n",
    "6. **Wang, Y., et al.** (2019). *Tree Transformer: Integrating Tree Structures into Self-Attention.* EMNLP.\n",
    "\n",
    "7. **Choi, J., Yoo, K.M., & Lee, S.** (2018). *Learning to Compose Task-Specific Tree Structures.* AAAI.\n",
    "\n",
    "8. **Hu, Z., et al.** (2021). *R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling.* ACL.\n",
    "\n",
    "9. **Drozdov, A., et al.** (2019). *Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders.* NAACL.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've implemented three types of recursive language models (TreeRNN, Tree-LSTM, Recursive Transformer), trained them on sentiment analysis, and visualized their learned representations. These models represent a fundamental approach to incorporating linguistic structure into neural networks for NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
